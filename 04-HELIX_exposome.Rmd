```{r, setup, include=FALSE}
knitr::opts_chunk$set(comment="", warning=FALSE, message=FALSE, cache=TRUE)
```

# Exposome data analysis in HELIX data. {#Exposome_HELIX_analysis}

The main aim of this **chapter** is to illustrate how to perform **exposome analyses** on a **multi-centre study** in a non-disclosive way using **DataSHIELD**. Particularly, we will cover how to apply some of the most frequently employed techniques in exposome analysis such is the case of **Exposome Wide Association Analysis (ExWAS)** or **variable selection with penalized regressions**. In order to illustrate the power and possibilities emanating from **DataSHIELD** in the context of exposome analysis, for this task, we will **reproduce** some of the analyses presented in the work of [@WAREMBOURG20191317], in which the association between a wide range of **prenatal and postnatal** exposures and **blood pressure (BP)** in children is investigated.

For this purpose, we will use the exposome data from the **HELIX project** previously described (\@ref(Exposome_HELIX_Data)), and the **DataSHIELD packages** designed for such purposes, the *"dsExposomeClient"* (CITA) from our research group, and the [**dsMTL (Federated Multi-Task Learning based on DataSHIELD)**](https://github.com/transbioZI/dsMTLBase) from [@Cao2021.08.26.457778].

In this **tutorial**, we will give a **general overview** of how to implement some of the main functions of these packages in the context of a **multi-centre exposome study**. Some **topics** covered in this **tutorial** include: 1) Setting up the **R environment** for working with these packages in **DataSHIELD**, 2) **Loading the HELIX showcase data** from an **Opal** server into the **DataSHIELD** environment, and 3) Application of **ExWAS** for multiple association analysis, and **Lasso regression** for variable-selection.

## Getting started.

In this section, we will describe how to **configure R and DataSHIELD** with the **needed packages** to carry out proposed of analyses in remote. First, it is necessary to **install** the *client-side* version of the following **DataSHIELD/Opal** integration packages.

```{r setup_1, eval=FALSE}
install.packages('DSOpal', dependencies=TRUE)
install.packages("DSI", dependencies=TRUE)
```

Make sure you also install the **DataSHIELD** *client-side* version of the packages _"dsBaseClient"_ and _"dsBase"_. The _"dsBaseClient"_ package needs to be a client which is version 6.0.0 or higher.

```{r setup_2, eval=FALSE}
install.packages("dsBase", dependencies=TRUE)
install.packages("dsBaseClient", repos = c("http://cran.datashield.org", "https://cloud.r-project.org/"), dependencies = TRUE)
```

Then, install the *client-side* version of the *"dsExposomeClient"* and *"dsMTL"* packages directly from _GitHub_.

```{r setup_3, eval=FALSE}
install.packages("devtools")
library("devtools")
devtools::install_github("transbioZI/dsMTLClient")
devtools::install_github('isglobal-brge/dsExposomeClient@1.1.2')
```

Once installations are complete, all the packages are **loaded** as usual.

```{r setup_4, echo=TRUE, results='hide', message=FALSE}
library(DSI)
library(DSOpal)
library(dsBaseClient)
library(dsMTLClient)
library(dsExposomeClient)
```

In this **tutorial**, we will use the [Opal BRGE site](https://datashield.isglobal.org/brge/ui/index.html) to illustrate how to perform mentioned analyses in **DataSHIELD**. Details for **accessing** the server can be found below. We will employ an user who have **DataSHIELD** permissions to Opal servers called *"dsuser*.

## Data formatting and manipulation in DataSHIELD.

In this section, we will cover how to **manipulate** and **prepare** **input data** with **DataSHIELD** according to the needs of the functions available in the *"dsExposomeClient"* and *"dsMTL"* packages.

We start by **creating the connection to the opal server** using an user who have **DataSHIELD permissions** to Opal servers (**dsuser**). Please, note that in our example, all datasets are hosted in the same Opal but each cohort sub-dataset is accessed separately. In the case each cohort dataset was available in a different opal, the way of login data would be the same but specifying the different user connection details.

```{r creating_conns_login_1, echo=TRUE, results='hide', message=FALSE}
builder <- DSI::newDSLoginBuilder()
builder$append(server = "BIB", url = "https://datashield.isglobal.org/repo",
               user =  "jrgonzalez", password = "Z2Z//B5",
               driver = "OpalDriver", profile = "rock-inma")
builder$append(server = "EDEN", url = "https://datashield.isglobal.org/repo",
               user =  "jrgonzalez", password = "Z2Z//B5",
               profile = "rock-inma")
builder$append(server = "KANC", url = "https://datashield.isglobal.org/repo",
               user =  "jrgonzalez", password = "Z2Z//B5",
               profile = "rock-inma")
builder$append(server = "MoBA", url = "https://datashield.isglobal.org/repo",
               user =  "jrgonzalez", password = "Z2Z//B5",
               profile = "rock-inma")
builder$append(server = "Rhea", url = "https://datashield.isglobal.org/repo",
               user =  "jrgonzalez", password = "Z2Z//B5",
               profile = "rock-inma")
builder$append(server = "INMASAB", url = "https://datashield.isglobal.org/repo",
               user =  "jrgonzalez", password = "Z2Z//B5",
               profile = "rock-inma")
logindata <- builder$build()
conns <- DSI::datashield.login(logins = logindata)
``` 
Next, we will **load** each of the **resources** available in the **Opal server**, corresponding to each **cohort exposomeSet**, using the _DSI::datashield.assign.resource()_ function and each of the connections to the server created in the previous code chunk. This way, we **assign all available resource objects in the Opal** to an R object called "resource" in the **Datashield remote session**. 

```{r assign_resources_login_2, echo=TRUE, results='hide', message=FALSE}
DSI::datashield.assign.resource(conns[1], "resource", "HELIX.postnatal_BIB")
DSI::datashield.assign.resource(conns[2], "resource", "HELIX.postnatal_EDE")
DSI::datashield.assign.resource(conns[3], "resource", "HELIX.postnatal_KAN")
DSI::datashield.assign.resource(conns[4], "resource", "HELIX.postnatal_MOB")
DSI::datashield.assign.resource(conns[5], "resource", "HELIX.postnatal_RHE")
DSI::datashield.assign.resource(conns[6], "resource", "HELIX.postnatal_SAB")
```
Now we have to **resolve the resources** and **retrieve the data in the remote session**. For that, we will use the _DSI::datashield.assign.expr()_ function. This function will assign the result of the execution of the argument "expr" to an R object called "exposome_set" in the **Datashield remote session**. In particular, we run the function _as.resource.object()_, which is the **DataSHIELD** function in charge of resolving resources (it coerces the resource to an internal data object that depends on the implementation of the object). As a result, we will get an **R object** (here named "exposome_Set") containing the "exposomeSets" files for all cohorts.

```{r resolve_resource_login_3, echo=TRUE, results='hide', message=FALSE}
DSI::datashield.assign.expr(conns = conns, symbol = "exposome_set",
                            expr = as.symbol("as.resource.object(resource)"))
```
Once we have loaded the resources, we have also to load each of the **tables with additional phenotypes** available for each cohort in the Opal server. For that purpose, we will use the _DSI::datashield.assign.table()_ function, which assigns **tables to an R object** in the **remote session** (here named "pheno")). As a result, the "pheno" object will contain **six data.frames** corresponding to each of the tables with extra phenotypes for cohorts.

```{r assign_tables_login_4, echo=TRUE, results='hide', message=FALSE}
DSI::datashield.assign.table(conns[1], "pheno", "HELIX.subclinical_cardio_BIB")
DSI::datashield.assign.table(conns[2], "pheno", "HELIX.subclinical_cardio_EDE")
DSI::datashield.assign.table(conns[3], "pheno", "HELIX.subclinical_cardio_KAN")
DSI::datashield.assign.table(conns[4], "pheno", "HELIX.subclinical_cardio_MOB")
DSI::datashield.assign.table(conns[5], "pheno", "HELIX.subclinical_cardio_RHE")
DSI::datashield.assign.table(conns[6], "pheno", "HELIX.subclinical_cardio_SAB")
```
To **verify previous steps were performed correctly**, we could use some **descriptive functions** on created objects. For example, we can check the names of the variables included in each object, and ensure this way that they exist on the remote session.

```{r descriptive_1, echo=TRUE, message=FALSE}
#Which is the class of the remote R objects "exposome_set" and "pheno":
ds.class("exposome_set")
ds.class("pheno")

#Get the number of exposures, phenotypes and individuals in each file:
ds.dim("pheno")
# si se lanza en el chunk de primeras falla, si se vuelve a lanzar funciona. ds.dim("exposome_set")

#Show the name of the columns available in the data.frames of the pheno object:
ds.colnames("pheno")

#Since the "exposome_set" object contains exposomeSet-type files for each cohort, the process for retrieving the names of variables is slightly different. For that purpose we have several functions from the _ds.ExposomeClient package_ called ds.phenotypeNames() and ds.exposome_variables(). #Get the name of the phenotypes available in the exposomeSet file for the EDEN cohort.
ds.phenotypeNames("exposome_set", conns)$EDEN
#Get the name of the exposures available in the exposomeSet file for the EDEN cohort.
ds.exposome_variables("exposome_set" , target="exposures")$EDEN
```
As we can observe from the previous descriptive, the **number of individuals** available in each of the cohort data.frames from the **"pheno" object** is **different** from the number of subjects available in each of the cohort exposomeSets from the **"exposome_set" object**. To work only with a single **exposomeSet** per **cohort**, we will now **merge** the additional phenotype data available in "pheno" to the outcomes already available in "exposome_set". To do that, there is a function from the **"dsExposomeClient"** package called _ds.addPhenoData2ExposomeSet()_. This function add new phenotype data contained on a data.frame to an ExposomeSet. The ExposomeSet may or may not already have phenotype data. If the data.frame contains a phenotype already present on the ExposomeSet, the server function will throw an exception. The pehnotypes data.frame has to contain an ID column which does not need to contain exactly the same individuals as the ExpressionSet, only the matching individuals will be updated, no new individuals will be introduced or removed from the ExposomeSet.

```{r data_manipulation_1, echo=TRUE, message=FALSE}
# We will employ the HelixID variable as the identifier to match subjects from each dataset.
ds.addPhenoData2ExposomeSet("exposome_set", "pheno", 
                            identifier_ExposomeSet = "HelixID", 
                            identifier_new_phenotypes = "HelixID")
```

We can check the process has been conducted successfully by exploring the **dimensions** of the new **exposomeSets files** created per cohort.

```{r descriptive_2, echo=TRUE, message=FALSE}
ds.dim("exposome_set")
#We can also check if there is metadata available in each expsomeSet file describing the families of exposomes.
ds.familyNames("exposome_set")
```

## Implementation of ExWAS in DataSHIELD.

As we said, the main aim of this **chapter** is to illustrate how to perform **exposome analyses** on a **multi-centre study** in a non-disclosive way using **DataSHIELD**. In this section we will cover how to apply **Exposome Wide Association Analysis (ExWAS)** in the prepared data from the **HELIX** project. For that, we will be reproducing some of the analyses conducted in [@WAREMBOURG20191317], in which the association between a wide range of **postnatal exposures** and **blood pressure (BP)** phenotypes in children was investigated. In the **childhood** period, this work identified a **bulk of external and internal exposures affecting** both diastolic and systolic **blood pressure** measurements (**_Figure 1_**). Here, for simplification, we will **replicate only some of the findings** reported for **systolic blood pressure** (**framed in red** in the **_Figure 1_**). Particularly, we will focus in the study of exposures belonging to the family of **"Organochlorine compounds (OCs)"**. 

<br>
```{r Figure1, echo=FALSE, fig.align = 'center', out.width = "90%", fig.cap = "Figure 1. Environmental-Wide Association Study Between Postnatal Exposome and Systolic and Diastolic Blood Pressure in the HELIX cohort (Warembourg C et al. (2019)[@WAREMBOURG20191317]."}
knitr::include_graphics(here::here("./fig", "charline_Bp.png"))
```
<br>

The **exact estimates and 95% confidence intervals (CIs)** obtained for these **exposures** are presented in \@ref(tab:tab1). Importantly, these analyses were properly adjusted by **confounders** including **child age, child height, child sex, child cohort, mother's age at the moment of birth, mother's BMI at the moment of birth and paternal country of origin**.

|                           | Systolic Blood Pressure           | Diastolic Blood Pressure          |
|---------------------------|----------------------------------:|----------------------------------:|
|Exposure                   | Beta [95% CI]       | P-value     | Beta [95% CI]        | P-value    |
|---------------------------|----------------------------------:|----------------------------------:|
|DDE                        |-2.1 [-2.92; -1.28]  | **0.00000** | -1.11 [-1.88; -0.34] | **0.00471**|
|HCB                        |-2.05 [-2.82; -1.29] | **0.00000** | -0.82 [-1.54; -0.10] | **0.02640**|
|PCB 153                    |-1.90 [-2.85; -0.95] | **0.00009** | -0.81 [-1.70; 0.09]  | 0.07759    |
|PCB 170                    |-1.73 [-2.73; -0.73] | **0.00068** | -0.93 [-1.86; 0.00]  | 0.05118    |
|PCBs (sum)                 |-1.93 [-2.94; -0.92] | **0.00018** | -0.91 [-1.85; 0.04]  | 0.05999    |
: (\#tab:tab1) Postnatal ExWAS and systolic and diastolic blood pressure (corrected p-value threshold=0.00068).

Before **replicating these findings through ExWAS**, it is advisable to do some basic **exploratory analysis** on the **outcome and confounders** to be used in the models. For that, we can use the function _ds.exposome_summary()_ from the package **"ds.ExposomeClient"**, which gives some basic summary statistics for both numeric and categorical variables. 

```{r show_families, eval=FALSE, hide=TRUE, message=FALSE}
# Retrieve basic summary statistics for the main outcome under study (systolic blood pressure).
ds.exposome_summary("exposome_set", "hs_bp_sys", conns)

# Retrieve basic summary statistics for the confounders to be included in the model.
ds.exposome_summary("exposome_set", "hs_child_age_days_None", conns)
ds.exposome_summary("exposome_set", "hs_c_height_None", conns)
ds.exposome_summary("exposome_set", "e3_sex_None", conns)
ds.exposome_summary("exposome_set", "h_cohort", conns)
ds.exposome_summary("exposome_set", "h_age_None", conns)
ds.exposome_summary("exposome_set", "h_mbmi_None", conns)
ds.exposome_summary("exposome_set", "h_native_None", conns)
```

From the previous chunk, we found **some problems** with the variable **parent's country of origin**, for which it seems there is no available data in some of the cohorts (*ERROR: "INVALID object!"*). Considering this, we will discard it from analysis, since it would give errors when incorporating into the models.

At this point, we are ready to **apply the ExWAS** on **systolic blood pressure** focusing in the particular set of **exposures** belonging the **OCs family**. For that, we have the _ds.exwas()_ function from the **"ds.ExposomeClient"** package. As we will see, there are several **ways** of applying an **ExWAS** in **DataSHIELD** (**_pooled vs. meta_**), and the chosen method will depend on the characteristics of each project. Both methods are based on fitting different **generalized linear models (GLMs)** for each **feature/exposure** when assessing association with the **phenotype of interest**. The “virtually” **pooled approach** is recommended when the user wants to analyze data from different sources and obtain results as if the data were located in a **single computer**. It should be noticed that this method is not recommended when data are not properly harmonized or comparable between cohorts, whatever the reason. On the other hand, the federated **meta-analysis approach** performs an **ExWAS on every study server** to later **meta-analyse** the results. Thanks to that, it overcomes the limitations raised when performing pooled analysis. 

In the next chunk of code, we present how to apply **both ExWAS approaches** to the **HELIX data**. Since in our example, for simplification, we want to focus only on **OCs exposures**, it will be vital that our exposomeSets count on metadata in a description file. This will allow us to use the **"exposure_family"** argument, in the _ds.exwas()_ function, in which we can indicate the family group of exposures to restrict our analysis to. Other arguments available in the _ds.exwas()_ function include the **"family"** argument, which refers to the type of link function to be used in the GLM (e.g., gaussian for continuous outcomes, binary for binary outomes, or poisson for counts). It's important to note that if the family argument does not match with the nature of the data available in the **phenotype**, the ExWAS will fail.

In the next chunk of code, we show how to apply ExWAS on our data following both approaches.

```{r exwas_analysis_1, echo=TRUE, message=FALSE}
#ExWAS with meta-analysis approach (without covariables). Estimated execution time: approx. 4.331925 mins
start_time <- Sys.time()
res.meta <- ds.exwas('hs_bp_sys ~ 1', type = "meta", 
                     exposures_family = "OCs", 
                     Set = 'exposome_set', family = 'gaussian')
end_time <- Sys.time()
end_time - start_time
res.meta
head(res.meta)

#ExWAS with meta-analysis approachpooled approach (without covariables). Estimated execution time: approx. 4.218055 mins
start_time <- Sys.time()
res.pooled <- ds.exwas('hs_bp_sys ~ 1', type = "pooled", 
                     exposures_family = "OCs", 
                     Set = 'exposome_set', family = 'gaussian')
end_time <- Sys.time()
end_time - start_time
res.pooled
head(res.pooled)
```

As we said, these previous formulas will fit **GLMs between the exposures and the phenotype** as follows:

```
    phenotype ~ exposure_1 + covar1 + ... + covarN
    phenotype ~ exposure_2 + covar1 + ... + covarN
    phenotype ~ exposure_3 + covar1 + ... + covarN
    ...
    phenotype ~ exposure_M + covar1 + ... + covarN
```

The model is written as a string, where the left side term is the **phenotype**, and the right term are the **covariates** or **confounders** (e.g. variables to be adjusted for). A **crude model** is fitted in our example using `phenotype ~ 1`. In the case of having **more covariates** proceed as: `phenotype ~ cov1 + cov2 + ... + covN`. 

To **visualize the results** from the **ExWAS**, the **ds.ExposomeClient** package has the function _ds.plotExwas()_. It takes the output of _ds.exwas()_ and creates two different visualizations depending on the argument **"type"**. In the next chunk we show how to generate a **Manhattan-like plot** with the p-values of the association between each exposure and the outcome colored by families of exposures. The vertical red line stands for the significant level corrected by the effective number of tests.
  
```{r plot_pvalues, fig.width=10, fig.height=12}
# One plot per cohort
ds.plotExwas(res.meta, type="manhattan")
# One plot for the whole population
ds.plotExwas(res.pooled, type="manhattan")
```

Given the nature of the "Manhattan plots", it should be noted that this kind of visualizations is more indicated when performing a big ExWAS study instead of focusing in a group of exposures. We can also generate a **plot showing the effects** (beta values) and their **confidence intervals** with:
  
```{r plot_effects, fig.width=10, fig.height=12}
# One plot per cohort
ds.plotExwas(res.meta, type="effect")
# One plot for the whole population
ds.plotExwas(res.pooled, type="effect")
```

Now, to replicate the findings from [@WAREMBOURG20191317], we will **repeat the pooled ExWAS approach** process but including all previously mentioned **adjusting covariates**, with the exception of the parent's country of origin, which presented some problems as we saw in previous chunks. As you can see, the order to adjust the models by cohort is give separately to the function, directly with the argument **"adjust.by.study"**.

```{r exwas_adjusted, eval = FALSE}
#Adjust by cohort and other covariates. Estimated execution time: approx. 5.670872 mins
start_time <- Sys.time()
res.pooled_adjusted <- ds.exwas("hs_bp_sys ~ hs_child_age_days_None + hs_c_height_None + e3_sex_None + h_age_None + h_mbmi_None", type = "pooled", 
                     adjust.by.study = TRUE,
                       exposures_family = "OCs", 
                     Set = "exposome_set", family = "gaussian")
end_time <- Sys.time()
end_time - start_time
res.pooled_adjusted
```

As it can be seen, we are able to validate the main findings presented for the paper, reporting an inverse association between the serum concentration of some of these compounds and the systolic blood pressure levels of children.

## Feature selection with penalized methods

In this section, we present how to perform **variable selection** with **penalized regression methods** in an exposome context. As previously mentioned, the _"dsMTL"_ **package** allows the implementation of a **federated version** of conventional **regularized regression techniques** (such as Lasso, Ridge or Elastic-net), both in the context of continuous or binary outcomes (linear and logistic regression, respectively). Here, we show how to apply these methods to the **HELIX** data, since variable selection is other important step in any exposome study.

Since most of the functions in the _"dsMTL"_ **package** require data of each cohort to be passed as **separate matrices** for the predictors and the outcome, in the next chunks of code we show how to transform the input **HELIX** dataset from the current **exposomeSet format** into two **matrices** with the required format in **DataSHIELD**. For that, we will start generating two **data.frames**, one for **exposure data** and one for the **outcome data**. We will use the function _ds.exposures_pData()_, which can extracts exposures, phenotypes or combined data and saves it to a data frame on the server side.

```{r get_exposures_cont, results='hide', warning=FALSE, message=FALSE}
#With the "name" argument, we can specify the name of the new R object to be created, if no name argument is provided, the new object will be named "set_table" where "set" is the inputted argument.
ds.exposures_pData('exposome_set',  
                   type = "exposures", exposures_type = "numeric",
                   name = 'table_exposures')
ds.exposures_pData('exposome_set',  
                   type = "phenotypes", exposures_type = "numeric",
                   name = 'table_phenotypes')

#we can check the objects created in the remote session for each cohort by typing
ds.ls()
```

Now we have two **different objects** for **exposures** and **phenotype data**, we can continue with the data preparation.

```{r data_preparation_lasso, results='hide', warning=FALSE, message=FALSE}
# We start by assigning the outcome Systolic Blood pressure to a new R object of the type numeric vector called "Y" in the server side.
ds.assign(toAssign="table_phenotypes$hs_bp_sys", newobj="Y", datasources = conns)
# Check if there is NA values in the phenotype.
ds.numNA("Y")
# Replace NA values by the cohort mean.
Y_means <- ds.mean(x = "Y",type = "split",save.mean.Nvalid = F,datasources = conns)$Mean.by.Study
ds.replaceNA(x = "Y", forNA = Y_means[,1], newobj = "Y_imp", datasources = conns)

# We extract the adjusting covariates from the phenotype dataset and assign them to a new dataframe names "X_confounders"
keep_conf <- which(ds.colnames('table_phenotypes')[[1]] %in% c("hs_child_age_days_None","hs_c_height_None","e3_sex_None","h_age_None","h_mbmi_None"))
ds.dataFrameSubset(df.name = "table_phenotypes",  V1.name = NULL,  V2.name = NULL,  Boolean.operator = NULL,keep.cols = keep_conf,  rm.cols = NULL,  keep.NAs = NULL,  newobj = "X_confounders",  datasources = conns, notify.of.progress = FALSE)

# We merge confounders data with exposures and assign the result to an R object calles "X".
ds.dataFrame(x = c("table_exposures","X_confounders"), completeCases = TRUE, newobj = "X", datasources = conns) 
  
# We again check dimensions and attribute names for the resulting objects.
ds.dim("X")
ds.class("Y")
ds.length("Y")
ds.colnames("X")[[1]][1:5]

# We coerce both Xs and Y into matrices. 
#ds.asMatrix(x.name = "Y", newobj = "Y", datasources = NULL)
ds.asMatrix(x.name = "table_exposures", newobj = "X", datasources = NULL)
ds.asMatrix(x.name = "Y", newobj = "Y", datasources = NULL)
ds.asMatrix(x.name = "Y_imp", newobj = "Y_imp", datasources = NULL)
# We assign both matrices to R objects in the client-side.
X="X"; Y="Y"; Y_imp="Y_imp"

```

Once we have the data in the right format, we can proceed to perform variable selection with penalized methods. **Available functions** in the package for such purpose include:

* _ds.LS_Lasso()_: Solver of **regression with Lasso**.
* _ds.LR_Lasso()_: Solver of **logistic regression** with **Lasso**.                
* _ds.Lasso_Train()_: Train a **regularization tree** with **Lasso** for a **sequence of penalty values** (than can be either provided by the user, or directly estimated from the data).
* _ds.Lasso_CVInSite()_: In-site **cross-validation** procedure for **selecting the optimal penalty**.

In all these functions, there is an argument called **"_C_"**, which is the **hyperparameter** for the **Ridge** regression L2 penalty. Thus, by tuning both the L1 penalty and the C argument (L2 term), one would be able to chose between a **Lasso**, **Ridge** or **Elastic-Net regression**. Likewise, there is an argument called **"_opts_"**, which allow controlling the optimization algorithm employed to minimize the sum of squared errors (SSE) (**objective function**). Additional details regarding the loss function implemented in these models can be found in the supplementary material of [@Cao2021.08.26.457778]. Within the **"_opts_"** argument we find:

* __"init"__: It determines the starting point.

* __"maxIter"__: It is the maximized iteration number.

* __"ter"__: It  refers to the termination rule used to determine the convergence of the algorithm. There are three termination rules available for *ds.lasso*. The first rule checks whether the current objective value was close enough to 0. The second rule investigates the last two objective values and checks whether the decline was close enough to 0. The third rule allowed the optimization to be performed for a certain maximum number of iterations.

* __"tol"__: It refers to the precision of the convergence and determines the termination of the program.

```{r analysis_1, warning=FALSE, message=FALSE}
#Default values for the opts argument.
opts=list();opts$init=0; opts$maxIter=10; opts$tol=0.01; opts$ter=2;
```

A **summary** of the **whole pipeline** that one could follow in **DataSHIELD** for performing **feature selection** with **Lasso regression** can be found in (**_Figure 2_**).

<br>
```{r Figure2, echo=FALSE, fig.align = 'center', out.width = "90%", fig.cap = "Figure 2. DataSHIELD for the implementation of penalized Lasso regression in a federated framework."}
knitr::include_graphics(here::here("./fig", "dslasso.jpg"))
```
<br>

### _ds.LS_Lasso()_ function.

With the _ds.LS_Lasso()_ function, we can apply the lasso solver of regression for a **particular** value of the **lambda** penalty. **Input** arguments available in the function include:
 
* _X_:	The design matrices of multiple cohorts.
* _Y_:	Label vectors of multiple cohorts.
* _lam_:	Input lambda value.
* _C_:	The hyper-parameter associated with L2 term.
* _opts_:	Options controlling the optimization procedure.
* _datasources_:	The connections of servers.
* _nDigits_:	The number of digits rounded for each number prepared for network transmission.
* _W_:	The current estimate of the variables (if available).

**_Note that if we set lam=0 and C>0 we will do Ridge regression, lam>0 and C=0 we will do Lasso, while any other combination of these both will be Elastic-Net._**

The **output** of this function comprises:

* The **vector of weights** estimated for input **predictors**. 
* The converged result of optimization. 
* The proximal point of W and the non-smooth part of objective.

In the next chunk, we apply this function to our **example dataset**, setting the value of the **lambda penalty** to "0.5". For this, we will test the association between the **174 predictors** and the continuous outcome **BMI Z-Score**. In case we were interested in modeling a **binary outcome**, we should use the _ds.LR_Lasso()_ function instead, which share exactly the same arguments and options. 

```{r analysis_2, warning=FALSE, message=FALSE}
# Estimated execution time: approx. 17.05 seconds.
set.seed(123)
m1=ds.LS_Lasso(X=X, Y=Y_imp, lam=0.5, C=0, opts, datasources=conns, nDigits=15)
# Show the number of selected variables by the model for the assessed lambda
sum(m1$w!=0)
```

In our example, with this **lambda penalty**, **41 variables** are selected as significant contributors to the outcome **BMI Z-Score**. Now, we can **plot** estimated coefficients from the model.

```{r analysis_3, warning=FALSE, message=FALSE}
plot(m1$w, xlab="index of coefficients", ylab="values")
```

Please, note that input variables are not standardized previous to the modeling in _ds.LS_Lasso()_, while, in the local solution _glmnet()_, the default argument is _standardize=TRUE_.

### _ds.Lasso_Train()_ function.

Instead of introducing a single value of lambda and fitting the model, a **whole lambda sequence** can be tested (either if it has been **defined by the user**, or if it has been **estimated from the data**). For that purpose, we must use the function _ds.Lasso_Train()_, which trains a whole **regularization tree** with **Lasso** for a **set** of **penalty values**.

Some **input** arguments available here, and not present in the previous function are:

* _type_: Regression(=regress) or classification(=classify).

* _nlambda_: The length of lambda sequence.

* _lam_ratio_: This is an option affecting how the lambda sequence is extracted from the data. It refers to the ratio: $min(lambda) / max(lambda)$. 

* _lambda_: The lambda sequence.

* _intercept_: Use intercept(=TRUE) or non-intercept(=FALSE) model.

The process of **lambdas estimation from the data** in this function is governed by two arguments: _"nlambda"_ and _"lam_ratio"_. First, the **lambda max** is determined as the smallest value of lambda for which no parameters are selected. In the next chunk, we show the code lines of the function source code that extract the **lambda max** value from the data.

```{r analysis_4, eval=FALSE, warning=FALSE, message=FALSE}
xys = DSI::datashield.aggregate(datasources, call("xtyDS", X, Y)) #matrix multiplication x * t(y) 
xys = rowSums(do.call(cbind, xys))/sum(nSubs)
xy_norm = max(abs(xys))
```

The determination of **λmin** and the number of grid points seems less principled. The **λmin** is calculated as $λmin=λratio∗λmax$, and then a grid of _n_ equally spaced points on the logarithmic scale between **λmin** and **λmax** is generated.

The **output** of the _ds.Lasso_Train()_ function is a list composed of:

* A **matrix** with as many columns as lambdas and as many rows as variables including the **estimated coefficients** for each variable-lambda.^
* A vector with the **sequence of lambdas** (either if they were estimated from data or introduced by the user).

Next, we show how to apply this function to our **case study**, estimating a **sequence of 4 lambda penalties** from the data. 

```{r analysis_5, warning=FALSE, message=FALSE}
# Estimated execution time: approx. 56.79 seconds.
m2=ds.Lasso_Train(X=X, Y=Y, type="regress", nlambda=4, lam_ratio=0.0001, C=0, opts=opts,datasources=conns, nDigits=15)
str(m2)
```

From the output of this function, the **number of selected variables** for each lambda value can also be extracted:

```{r analysis_6, warning=FALSE, message=FALSE}
fun <- function(x) { length(which(x != 0)) }
apply(m2$ws,2,fun)
```

And we can plot the results (**regularization tree**). If the estimation of the **λmax** penalty has been performed correctly, all horizontal lines in the plot should begin with _w=0_, since, as we said before, by definition, the **λmax** is the smallest value of lambda for which no parameters are selected.

```{r analysis_7, warning=FALSE, message=FALSE}
matplot(t(m2$ws), type = "l", main="solution Path", xlab = "lambda", ylab = "coefficients")
```

_Please, note that the values in the **X** axis refer to the appearance order of each lambda in the lambda sequence vector, i.e., Lam1=18.58,  Lam2=0.86,  Lam3=0.04 and Lam4=0._

In the case of the **lambda sequence** is **defined by the user**, we would have:

```{r analysis_9, warning=FALSE, message=FALSE}
# Estimated execution time: approx. 36.23 seconds.
m3=ds.Lasso_Train(X=X, Y=Y, type="regress", nlambda=2, lambda=c(0.2,0.6), C=0, opts=opts,datasources=conns, nDigits=15)
#Show number of selected variables and plot coefficients.
apply(m3$ws,2,fun)
matplot(t(m3$ws), type = "l", main="solution Path", xlab = "lambda", ylab = "coefficients")
```

In the next table (Table 2), we show the main similarities and differences between the function _ds.Lasso_Train()_ and the local alternative _glmnet()_.

| <b>Table 2. Similarities and differences between the function _ds.Lasso_Train()_ and the local alternative _glmnet()_.</b>
ds.Lasso_Train()| glmnet()
----------------|----------------
X | X
Y | Y
lam | lambda
C | alpha
nlambda | nlambda
lam_ratio | lambda.min.ratio
intercept | intercept
Non Available | standardize
Non Available | penalty.factor

An **important drawback** of the current version of the _ds.Lasso_Train()_ function, is that it **does not allow to force the inclusion of covariates in models**, which in _glmnet()_ is achieved through the argument _"penalty.factor"_ by reducing to 0 the penalty to be applied to their coefficients. Other **important difference** is the type of **optimization algorithm** employed for the estimation of coefficients. In comparison to _glmnet()_, which implements the coordinate descent [@Wright2015], _ds.Lasso_Train()_ implements an optimization procedure based on the proximal algorithm framework [@OPT-003].

### _ds.Lasso_CVInSite()_ function.

As in _glmnet()_, _ds.Lasso_ also allows the selection of the **optimal lambda** by a **k-fold cross-validation procedure**. This is done in _"dsMTL"_ with the function _ds.Lasso_CVInSite()_, which has practically the same arguments than previous functions, except for the _"nfolds"_ argument, referring to the number of folds for the CV procedure. 

From the application of this function to our **case study**, we can estimate a lambda sequence of 5 values with a 5-fold CV procedure:

```{r analysis_10, warning=FALSE, message=FALSE}
# Estimated execution time: approx. 8.17 mins.
cvResult=ds.Lasso_CVInSite(X=X, Y=Y, type="regress", lam_ratio=0.1, nlambda=5, opts=opts, C=0, datasources=conns, nDigits=4, nfolds=5)
# Boxplot showing the averaged MSE obtained for lambda values over folds
boxplot(cvResult$mse_fold, names=as.character(round(colMeans(cvResult$lam_seq), 3)), 
        xlab="averaged lambda over folds", 
        ylab="mean squared error")
```

From this model, we could select the **optimal lambda value** directly from the **output** with the _"cvResult$lambda.min"_ argument, and then **fit a new model** with optimal hyperparameters.

```{r analysis_11, warning=FALSE, message=FALSE}
# Estimated execution time: approx. 1.40507 mins.
# Optimal lambda from CV
cvResult$lambda.min
# Training a new model with selected hype-parameter
m4=ds.Lasso_Train(X=X, Y=Y, type="regress", lambda=cvResult$lambda.min, nlambda=5, 
                   opts=opts, C=0, datasources=conns, nDigits=4)
# Number of selected variables and their estimated coefficients
apply(m4$ws,2,fun)
plot(m4$ws, xlab="index of coefficients", ylab="values")

```

As previously mentioned, all these functions can be adapted for **Ridge** and **Elastic-Net** regression by properly tuning the _"C"_ argument.

```{r logout}
datashield.logout(conns)
```
